{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsrunFvIkoPN",
    "outputId": "27fbc249-726a-42eb-9d7c-9bbc023b35a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio neptune-client tqdm --quiet > /dev/null \n",
    "!pip install transformers==2.1.1 folium==0.2.1 pytorch-lightning --quiet > /dev/null\n",
    "!apt install git git-lfs > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D__dL3ILljKC"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/karlfroldan/prototype.git\n",
    "!git clone https://huggingface.co/microsoft/codebert-base\n",
    "!mv prototype/* . \n",
    "!rm -rf prototype \n",
    "!mv codebert-base codebert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mu9BtbP6nrDN"
   },
   "outputs": [],
   "source": [
    "%cd codebert \n",
    "!git lfs install\n",
    "!git lfs pull \n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99ec1ZRImN_4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
    "\n",
    "import neptune.new as neptune\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "from prototype_dataloader import get_datasets\n",
    "\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "import warnings\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'using device: {device}')\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"\n",
    "    Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "# Set the RNG\n",
    "seed_everything(1729)\n",
    "\n",
    "\n",
    "#torch.manual_seed(1729) # A Tribute to Srinavasa Ramanujan\n",
    "# quicksort          0\n",
    "# mergesort          0\n",
    "# selectionsort      0\n",
    "# insertionsort      0\n",
    "# bubblesort         1\n",
    "# linearsearch       0\n",
    "# binarysearch       0\n",
    "# linkedlist         0\n",
    "# hashmap            0\n",
    "def get_labels(arr):\n",
    "    cols = [\"quicksort\", \"mergesort\", \"selectionsort\", \"insertionsort\", \"bubblesort\", \n",
    "            \"linearsearch\", \"binarysearch\", \"linkedlist\", \"hashmap\"]    \n",
    "    return list(map(lambda tup: tup[0], \n",
    "                    filter(lambda tup: tup[1] == 1, \n",
    "                           zip(cols, arr.tolist()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO8pLokAPNXd"
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciDV0xPEmrVm"
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"./codebert\")\n",
    "model = RobertaModel.from_pretrained(\"./codebert\")\n",
    "\n",
    "data_csv = pd.read_csv(\"prototype.csv\")\n",
    "# split=0.1 => split=1.0 because we want to use k-fold cross validation instead\n",
    "#train_data, test_data = get_datasets(data_csv, tokenizer, split=0.1, data_folder='./data/prototype')\n",
    "data, _empty = get_datasets(data_csv, tokenizer, split=1.0, data_folder='./data/prototype')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUqDe900oToJ"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOz4N9WFng4M"
   },
   "outputs": [],
   "source": [
    "class OurModel(pl.LightningModule):\n",
    "    def __init__(self, codebert, loss, input=393_216, hidden=None, labels=9, train_rate=1e-3, device='cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = codebert\n",
    "        \n",
    "        # Disable the gradients of codebert\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.train_rate = train_rate\n",
    "        \n",
    "        self.fc1 = nn.Linear(768 * 512, 420)\n",
    "        self.hidden_is_none = hidden is None\n",
    "        last = 420\n",
    "        if hidden is not None:\n",
    "            self.hidden = []\n",
    "            for i in hidden:\n",
    "                n = nn.Linear(last, i).cuda()\n",
    "                \n",
    "                self.hidden.append(n)\n",
    "                last = i\n",
    "        \n",
    "        self.output = nn.Linear(last, labels)\n",
    "\n",
    "    def get_preds(self, y):\n",
    "        return (y >= 0.5).long()\n",
    "      \n",
    "    def get_preds_numpy(self, y):\n",
    "        return (y >= 0.5).astype(int)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (out, mask) = self.transformer(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        if not self.hidden_is_none:\n",
    "            for layer in self.hidden:\n",
    "                out = F.relu(layer(out))\n",
    "        # Instead, we need to ensure that we add a sigmoid layer\n",
    "        # when training the model.\n",
    "        return self.output(out) \n",
    "        #out = self.output(out)\n",
    "        #return #F.sigmoid(out)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.train_rate)\n",
    "        return optimizer\n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        X, y = train_batch\n",
    "        X = X['input_ids']\n",
    "        y_hat = self(X)\n",
    "        \n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        X, y = val_batch\n",
    "        X = X['input_ids']\n",
    "        y_hat = self(X)\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        y_hat_sigmoid = torch.sigmoid(y_hat)\n",
    "        self.log('validation loss', loss)\n",
    "        #subset_acc = self.subset_accuracy(y_hat_sigmoid, y) \n",
    "\n",
    "        # Transfer them to the CPU\n",
    "\n",
    "        y_cpu = y.squeeze().cpu().detach().numpy()\n",
    "        y_hat_sigmoid_cpu = self.get_preds(y_hat_sigmoid).squeeze().cpu().detach().numpy()\n",
    "\n",
    "        hamming = hamming_loss(y_cpu, y_hat_sigmoid_cpu)\n",
    "\n",
    "        f1_micro = f1_score(y_cpu, y_hat_sigmoid_cpu, average='micro', zero_division=1)\n",
    "        f1_macro = f1_score(y_cpu, y_hat_sigmoid_cpu, average='macro', zero_division=1)\n",
    "        self.log('hamming loss', hamming)\n",
    "        self.log('Micro F1', f1_micro)\n",
    "        self.log('Macro F1', f1_macro)\n",
    "\n",
    "        \n",
    "\n",
    "        # self.log('True labels', y_cpu)\n",
    "        # self.log('Predicted labels', y_hat_sigmoid_cpu)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCMiPURIogRe"
   },
   "source": [
    "### The Loss Function\n",
    "\n",
    "The criterion that this model will use is the **Focal Loss** which is defined as an extension of the **Cross-entropy loss**. \n",
    "\n",
    "We know that Cross-entropy loss is defined as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSTYk-SHoea_"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=4, alpha=0.1, device='cuda'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1 - alpha])\n",
    "        if device == 'cuda':\n",
    "          self.alpha = self.alpha.cuda()\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, y_hat, y):\n",
    "        epsilon = 1e-4\n",
    "        y_prime = y.type(torch.float32)\n",
    "        b = self.bce(y_hat, y_prime)\n",
    "\n",
    "        alpha_t = self.alpha.gather(0, y.data.view(-1)).reshape(-1, 9)\n",
    "        p_t = torch.exp(-b + epsilon)\n",
    "        \n",
    "        F_loss = alpha_t * (1 - p_t) ** self.gamma * b\n",
    "        return F_loss.mean()\n",
    "\n",
    "class CEWithLogitsLoss(nn.Module):\n",
    "  def __init__(self, device='cuda'):\n",
    "        super(CEWithLogitsLoss, self).__init__()\n",
    "        weights = [1,1,1,1,1,1,1,1,1]\n",
    "        if device == 'cuda':\n",
    "          class_weights = torch.FloatTensor(weights).cuda()\n",
    "        class_weights = torch.FloatTensor(weights)\n",
    "        self.bce = nn.BCEWithLogitsLoss(weight = class_weights, reduction='none')\n",
    "\n",
    "  def forward(self, y_hat, y):\n",
    "    y_prime = y.type(torch.float32)\n",
    "    b = self.bce(y_hat, y_prime).mean()\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYLVpB-polxz"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULWx-yg5ojrt"
   },
   "outputs": [],
   "source": [
    "focal_loss = FocalLoss(gamma=5, alpha=0.25, device='cuda')\n",
    "crossentropy_loss = CEWithLogitsLoss(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-3X7haB0CpR"
   },
   "outputs": [],
   "source": [
    "# Some important stuff in our k-fold validation\n",
    "kfold = KFold(n_splits=8, shuffle=True)\n",
    "print(f'kfold n-splits: {kfold.get_n_splits(data)}')\n",
    "\n",
    "# Manual. Values from 0 to 8.\n",
    "which_split = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyAJ5HjLopdZ"
   },
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(gpus=1, precision=32, max_epochs=5, log_every_n_steps=15)\n",
    "# trainer.fit(m, trainset, valset)\n",
    "\n",
    "for k_split_idx, (train_idxs, test_idxs) in enumerate(kfold.split(data)):\n",
    "\n",
    "    # If our `which_split` value is not equal the current `k_split_idx`, we skip\n",
    "    # this for loop iteration. This is because we wanna save memory when training.\n",
    "    # We can't afford to train twice.\n",
    "    if (k_split_idx != which_split):\n",
    "        continue;\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idxs)\n",
    "    test_sampler = SubsetRandomSampler(test_idxs)\n",
    "    #print(\"train_idxs \", len(train_idxs), \"test_idxs \", len(test_idxs))\n",
    "    trainset = DataLoader(data, batch_size=32, sampler=train_sampler)\n",
    "    # We set this as global for evaluation later on\n",
    "    global valset, m\n",
    "    valset = DataLoader(data, batch_size=32, sampler=test_sampler)\n",
    "    neptune_logger = NeptuneLogger(\n",
    "        api_key=\"\",  # replace with your own\n",
    "        project=\"pancit-canton/Optimus\",  # \"<WORKSPACE/PROJECT>\"\n",
    "        #tags=[\"training\", \"resnet\"],  # optional\n",
    "    )\n",
    "\n",
    "    m = OurModel(model, loss=crossentropy_loss, train_rate=1e-5, hidden=None, device=device)\n",
    "    trainer = pl.Trainer(gpus=1, precision=16, max_epochs=2, log_every_n_steps=6, logger=neptune_logger)\n",
    "    trainer.fit(m, trainset, valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wcL1nFSqvJ9"
   },
   "source": [
    "The hamming loss is defined as \n",
    "$$\n",
    "\\frac{1}{|N|\\cdot|L|}\\sum_{i=1}^{|N|}\\sum_{j=1}^{|L|}\\left(\\widehat{y}_{i,j}\\oplus y_{i,j}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCSDcryUF7dV"
   },
   "outputs": [],
   "source": [
    "prediction = []\n",
    "real = []\n",
    "subset_acc = 0\n",
    "subset_accuracy = lambda y_hat, y: torch.all((y == y_hat)).float()\n",
    "subset_pred = lambda y_hat, y: (torch.all(get_preds(y_hat) == y)).float()\n",
    "get_preds = lambda ys : (ys >= 0.5).long()\n",
    "        \n",
    "for X, y in tqdm(valset):\n",
    "    X = X['input_ids']\n",
    "    y_hat = torch.sigmoid(m(X))\n",
    "    prediction.append(get_preds(y_hat).detach().numpy())\n",
    "    subset_acc += subset_pred(y_hat, y)\n",
    "    real.append(y.detach().numpy())\n",
    "\n",
    "subset_acc /= (len(valset) * 9)\n",
    "print(f'Subset accuracy: {subset_acc}')\n",
    "\n",
    "prediction = np.vstack(prediction)\n",
    "real = np.vstack(real)\n",
    "\n",
    "import pickle as pkl \n",
    "\n",
    "for fname, array in zip(['prediction.pkl', 'real.pkl'], [prediction, real]):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pkl.dump(array, f)\n",
    "        print(f'Dumped {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nq0caebk--x8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Focal_Loss_Initial_Training-3 (use this).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
